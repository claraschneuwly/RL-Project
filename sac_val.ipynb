{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_between_vectors(v1, v2):\n",
    "    dot_product = v1[0] * v2[0] + v1[1] * v2[1]\n",
    "    magnitude_v1 = math.sqrt(v1[0]**2 + v1[1]**2)\n",
    "    magnitude_v2 = math.sqrt(v2[0]**2 + v2[1]**2)\n",
    "    \n",
    "    if magnitude_v1 != 0 and magnitude_v2 != 0:\n",
    "        cos_theta = dot_product / (magnitude_v1 * magnitude_v2)\n",
    "        if cos_theta > 1:\n",
    "                cos_theta = 1\n",
    "        elif cos_theta < -1:\n",
    "            cos_theta = -1\n",
    "        angle_radians = math.acos(cos_theta)\n",
    "        return angle_radians\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def normalize_vector_from_point(x, y, x0, y0):\n",
    "    # Calcul des composantes du vecteur\n",
    "    vector_x = x - x0\n",
    "    vector_y = y - y0\n",
    "    \n",
    "    # Calcul de la magnitude\n",
    "    magnitude = math.sqrt(vector_x**2 + vector_y**2)\n",
    "    \n",
    "    # Normalisation\n",
    "    if magnitude != 0:\n",
    "        normalized_x = vector_x / magnitude\n",
    "        normalized_y = vector_y / magnitude\n",
    "        return (normalized_x, normalized_y)\n",
    "    else:\n",
    "        return (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FluidMechanicsEnv:\n",
    "    \n",
    "    class Wave:\n",
    "        def __init__(self, a, T, k) :\n",
    "            self.a = a                  # Wave amplitude\n",
    "            self.T = T                  # Wave period\n",
    "            self.omega = 2 * np.pi / T  # Wave frequency\n",
    "            self.k = .1   \n",
    "\n",
    "    class Wind:\n",
    "        def __init__(self, Ux, Uy, alpha, sigma) :\n",
    "            self.Ux = Ux                  # Wave amplitude\n",
    "            self.Uy = Uy                  # Wave period\n",
    "            self.alpha = alpha            # Wave frequency\n",
    "            self.sigma = sigma\n",
    "\n",
    "    def __init__(self, a, T, k,  Ux, Uy, alpha, sigma, x_goal, y_goal, pos0, theta0, dist_threshold=0.1, max_steps=1000, ocean=False, dt=1, max_thrust_speed=1):\n",
    "        self.t = 0\n",
    "        self.wave = self.Wave(a, T, k)\n",
    "        self.wind = self.Wind(Ux, Uy, alpha, sigma)\n",
    "        self.max_steps = max_steps\n",
    "        self.dist_threshold = dist_threshold\n",
    "        self.max_x, self.min_x = 100 , -100  # agent has drifted too far, admit defeat\n",
    "        self.max_y, self.min_y = 100 , -100 # agent has drifted too far, admit defeat\n",
    "        self.x_goal, self.y_goal, self.z_goal = x_goal, y_goal, 0 # coordinates of goal\n",
    "        self.dir_goal = normalize_vector_from_point(self.x_goal, self.y_goal, 0, 0)\n",
    "        self.done = False\n",
    "        self.goal_reached = False\n",
    "        self.steps_count = 0\n",
    "        self.sum_reward = 0\n",
    "        self.all_actions = []\n",
    "        self.pos = pos0\n",
    "        self.theta = theta0\n",
    "        self.vel = np.array([0, 0, 0]).astype(np.float32)\n",
    "        self.thrust = 0 # [0; 1]\n",
    "        self.rudder = 0.0 # [-pi/4; pi/4]\n",
    "        self.action = np.array([0, 0])\n",
    "        self.u_history = []\n",
    "        self.v_history = []\n",
    "        self.straight = False\n",
    "        self.alpha = 0.1\n",
    "        self.ocean = ocean\n",
    "        if self.ocean:\n",
    "            self.state_dim = 9 # u_water, v_water\n",
    "        else:\n",
    "            self.state_dim = 7  # x, y, z. \n",
    "        self.action_dim = 2  # thrust, rudder angle\n",
    "        self.dt = dt\n",
    "        self.max_thrust_speed = max_thrust_speed\n",
    "\n",
    "        if self.ocean:\n",
    "            self.observation_space = spaces.Box(low=-100, high=100, shape=(9,), dtype=np.float32)\n",
    "        else:\n",
    "            self.observation_space = spaces.Box(low=-100, high=100, shape=(7,), dtype=np.float32)\n",
    "        \n",
    "        self.action_space = spaces.Box(low=np.array([0, -np.pi/4]), high=np.array([max_thrust_speed, np.pi/4]), dtype=np.float32)\n",
    "\n",
    "    def water_surface_level(self, pos) :\n",
    "        x, _, _ = pos\n",
    "        eta = self.wave.a * np.sin(self.wave.omega * self.t - self.wave.k * x)\n",
    "        return eta\n",
    "\n",
    "    def water_speed(self, pos) :\n",
    "        x, y, z = pos\n",
    "        eta = self.water_surface_level(pos)\n",
    "\n",
    "        u_swell = self.wave.a * self.wave.omega * np.exp(self.wave.k * z) * np.sin(self.wave.omega * self.t - self.wave.k * x)\n",
    "        w_swell = self.wave.a * self.wave.omega * np.exp(self.wave.k * z) * np.cos(self.wave.omega * self.t - self.wave.k * x)\n",
    "        \n",
    "        u_wind = np.random.normal(self.wind.Ux, self.wind.sigma) * np.exp(self.wind.alpha * (z-self.wave.a))\n",
    "        v_wind = np.random.normal(self.wind.Uy, self.wind.sigma) * np.exp(self.wind.alpha * (z-self.wave.a))\n",
    "\n",
    "        # u = u + np.random.normal(0, noise, u.shape)\n",
    "        # v = v + np.random.normal(0, noise, v.shape)\n",
    "        # w = w + np.random.normal(0, noise, w.shape)\n",
    "\n",
    "        return u_swell + u_wind, v_wind, w_swell\n",
    "\n",
    "    def inertia(self, lag = 3):\n",
    "\n",
    "        if len(self.u_history) > 0 :\n",
    "            k = np.minimum(lag, len(self.u_history))\n",
    "            coefs = np.array([1 / (4 ** (i + 1)) for i in reversed(range(k))])\n",
    "            u = (self.u_history[-k:] * coefs).sum()\n",
    "            v = (self.v_history[-k:] * coefs).sum()\n",
    "\n",
    "        else :\n",
    "            u, v = 0, 0\n",
    "\n",
    "        return np.array([u, v, 0])\n",
    "    \n",
    "    def update_pos(self, action):\n",
    "        # Sets agent action\n",
    "        self.thrust = action[0]*self.max_thrust_speed\n",
    "        self.rudder = action[1]\n",
    "    \n",
    "        # Find the water velocity at agent position\n",
    "        x, y, z = self.pos\n",
    "        u, v, w = self.water_speed(self.pos)\n",
    "        self.vel = np.array([u, v, w])\n",
    "\n",
    "        # Add inertia to the agent's velocity\n",
    "        self.vel += self.inertia()\n",
    "\n",
    "        # Perform agent action\n",
    "        self.theta += self.rudder \n",
    "        self.theta %= (2*np.pi) # Update agent's orientation from rudder angle\n",
    "        u_action = self.thrust * np.cos(self.theta)\n",
    "        v_action = self.thrust * np.sin(self.theta)\n",
    "        self.vel += np.array([u_action, v_action, 0])\n",
    "\n",
    "        # Update velocity history\n",
    "        self.u_history.append(u)\n",
    "        self.v_history.append(v)\n",
    "\n",
    "        # Update agent position\n",
    "        x += self.vel[0]*self.dt\n",
    "        y += self.vel[1]*self.dt\n",
    "        z = self.water_surface_level((x, y, z))\n",
    "\n",
    "        # Lucas' alignement checks (Lucas double check stp)\n",
    "        self.dir_goal = normalize_vector_from_point(self.x_goal, self.y_goal, x, y)\n",
    "        if not self.straight and angle_between_vectors(self.dir_goal, (np.sin(self.theta), np.cos(self.theta))) < 2*self.alpha:\n",
    "            self.straight = True\n",
    "\n",
    "        return np.array([x, y, z])\n",
    "    \n",
    "    def get_reward(self):\n",
    "        \n",
    "        # Calculate euclidian dist to goal Without z coord\n",
    "        goal_pos = np.array([self.x_goal, self.y_goal])\n",
    "        dist_to_goal = np.linalg.norm(np.array(self.pos[:2]) - goal_pos)\n",
    "        dist_to_dir = angle_between_vectors(self.dir_goal, (np.sin(self.theta), np.cos(self.theta)))/np.pi\n",
    "        ##reward = - (dist_to_goal/100 + np.float64(dist_to_dir))/50\n",
    "\n",
    "        #TODO: write parametrize reward function based on reward position\n",
    "\n",
    "        reward = - (dist_to_goal/5000 + (np.exp((1 + np.float64(dist_to_dir))) - 1)/200)\n",
    "        if dist_to_goal <= self.dist_threshold:\n",
    "            reward += 10\n",
    "        if self.straight and angle_between_vectors(self.dir_goal, (np.sin(self.theta), np.cos(self.theta))) >= self.alpha:\n",
    "            reward -= 0\n",
    "        return reward\n",
    "    \n",
    "    def success(self):\n",
    "        \"\"\"Returns True if x,y is near enough goal\"\"\"\n",
    "        goal_pos = np.array([self.x_goal, self.y_goal])\n",
    "        dist_to_goal = np.linalg.norm(np.array(self.pos[:2]) - goal_pos)\n",
    "        if  dist_to_goal <= self.dist_threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def admit_defeat(self):\n",
    "        \"\"\"Returns True if the agent has drifted too far away from goal\"\"\"\n",
    "        if self.pos[0] > self.max_x or self.pos[0] < self.min_x or self.pos[1] > self.max_y or self.pos[1] < self.min_y:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def step(self, action) :\n",
    "        \n",
    "        self.pos = self.update_pos(action)\n",
    "        self.reward = self.get_reward()\n",
    "        self.sum_reward += self.reward\n",
    "        self.steps_count += 1\n",
    "        self.all_actions += [action]\n",
    "        self.t += self.dt\n",
    "        if self.success():\n",
    "            self.done = True\n",
    "            self.goal_reached = True\n",
    "\n",
    "        elif self.admit_defeat() or self.steps_count > self.max_steps:\n",
    "            self.done = True\n",
    "        \n",
    "        if self.ocean: \n",
    "            # TODO: Add noise to observation if algo works fine with perfect estimate of wave and winds speed\n",
    "\n",
    "            # TODO: separate wind and water component\n",
    "\n",
    "            u_water, v_water, _ = self.water_speed(self.pos)\n",
    "            return np.concatenate((self.pos, np.array([np.cos(self.theta), np.sin(self.theta)]),  np.array([self.x_goal, self.y_goal]), np.array([u_water, v_water]))), self.reward, self.sum_reward, self.done, self.steps_count, self.all_actions\n",
    "        \n",
    "        else:\n",
    "            return np.concatenate((self.pos, np.array([np.cos(self.theta), np.sin(self.theta)]),  np.array([self.x_goal, self.y_goal]))), self.reward, self.sum_reward, self.done, self.steps_count, self.all_actions\n",
    "        \n",
    "    def reset(self):\n",
    "\n",
    "        self.rudder = 0\n",
    "        self.thrust = 0  \n",
    "        self.pos = np.array([0, 0, 0])\n",
    "        self.done = False\n",
    "        self.goal_reached = False\n",
    "        self.steps_count = 0\n",
    "        self.sum_reward = 0\n",
    "        self.t = 0\n",
    "        # Statement: by randomly initilizing theta, we learn beter how to adjust/turn round to reach goal\n",
    "        self.theta = np.random.uniform(0, 2*np.pi) # if statement false: self.theta = 0\n",
    "        self.dir_goal = normalize_vector_from_point(self.x_goal, self.y_goal, 0, 0)\n",
    "        self.straight = False\n",
    "\n",
    "        if self.ocean: \n",
    "            # TODO: Add noise to observation if algo works fine with perfect estimate of wave and winds speed\n",
    "\n",
    "            # TODO: separate wind and water component\n",
    "\n",
    "            u_water, v_water, _ = self.water_speed(self.pos)\n",
    "            return np.concatenate((self.pos, np.array([np.cos(self.theta), np.sin(self.theta)]),  np.array([self.x_goal, self.y_goal]), np.array([u_water, v_water])))\n",
    "        else:\n",
    "            return np.concatenate((self.pos, np.array([np.cos(self.theta), np.sin(self.theta)]),  np.array([self.x_goal, self.y_goal])))\n",
    "        \n",
    "\n",
    "\n",
    "env = FluidMechanicsEnv(a=0.1, # range 0.1, 0.5, 1, 2, 5\n",
    "                        T=1, # wave period, range 10 to 20\n",
    "                        k=0.1, #wave number m^-1: 0.05 to 0.5\n",
    "                        Ux=0, #wind x component: -2 to 2\n",
    "                        Uy=0, \n",
    "                        alpha=1, # vertical wind decay: around 1\n",
    "                        sigma=0, # noise wind parameter: around 10% wind speed\n",
    "                        x_goal=4, \n",
    "                        y_goal=4, \n",
    "                        pos0=np.array([0, 0, 0]), \n",
    "                        theta0=0,\n",
    "                        dist_threshold=0.2, \n",
    "                        max_steps=200, \n",
    "                        ocean=False, # if false: still water env. If true: ocean like env\n",
    "                        dt=1, # time step. For now keep 1, we could go smaller\n",
    "                        max_thrust_speed = 1# Robot's speed at 100% thrust \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    def thunk():\n",
    "        env = FluidMechanicsEnv(\n",
    "            a=0.1, T=1, k=0.1, Ux=0, Uy=0, alpha=1, sigma=0, \n",
    "            x_goal=4, y_goal=4, pos0=np.array([0, 0, 0]), theta0=0,\n",
    "            dist_threshold=0.2, max_steps=200, ocean=False, dt=1, max_thrust_speed=1\n",
    "        )\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"Hopper-v4\"\n",
    "    \"\"\"the environment id of the task\"\"\"\n",
    "    total_timesteps: int = 5010\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    buffer_size: int = int(1e6)\n",
    "    \"\"\"the replay memory buffer size\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    tau: float = 0.005\n",
    "    \"\"\"target smoothing coefficient (default: 0.005)\"\"\"\n",
    "    batch_size: int = 256\n",
    "    \"\"\"the batch size of sample from the reply memory\"\"\"\n",
    "    learning_starts: int = 5e3\n",
    "    \"\"\"timestep to start learning\"\"\"\n",
    "    policy_lr: float = 3e-4\n",
    "    \"\"\"the learning rate of the policy network optimizer\"\"\"\n",
    "    q_lr: float = 1e-3\n",
    "    \"\"\"the learning rate of the Q network network optimizer\"\"\"\n",
    "    policy_frequency: int = 2\n",
    "    \"\"\"the frequency of training policy (delayed)\"\"\"\n",
    "    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.\n",
    "    \"\"\"the frequency of updates for the target nerworks\"\"\"\n",
    "    noise_clip: float = 0.5\n",
    "    \"\"\"noise clip parameter of the Target Policy Smoothing Regularization\"\"\"\n",
    "    alpha: float = 0.2\n",
    "    \"\"\"Entropy regularization coefficient.\"\"\"\n",
    "    autotune: bool = True\n",
    "    \"\"\"automatic tuning of the entropy coefficient\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO LOGIC: initialize agent here:\n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod() + np.prod(env.action_space.shape), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(env.observation_space.shape).prod()\n",
    "np.prod(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod(), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mean = nn.Linear(256, np.prod(env.action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(256, np.prod(env.action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        #print('action', action)\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        #print('logProb', log_prob)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        #print('log_prob', log_prob)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env setup\n",
    "\n",
    "# envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])\n",
    "# assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "# max_action = float(envs.single_action_space.high[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = make_env()()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(envs).to(device)\n",
    "qf1 = SoftQNetwork(envs).to(device)\n",
    "qf2 = SoftQNetwork(envs).to(device)\n",
    "qf1_target = SoftQNetwork(envs).to(device)\n",
    "qf2_target = SoftQNetwork(envs).to(device)\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "qf2_target.load_state_dict(qf2.state_dict())\n",
    "q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e9)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic entropy tuning\n",
    "if args.autotune:\n",
    "    target_entropy = -torch.prod(torch.Tensor(envs.action_space.shape).to(device)).item()\n",
    "    log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    alpha = log_alpha.exp().item()\n",
    "    a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)\n",
    "else:\n",
    "    alpha = args.alpha\n",
    "\n",
    "envs.observation_space.dtype = np.float32\n",
    "\n",
    "rb = ReplayBuffer(envs.observation_space.shape[0], envs.action_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5002\n",
      "5004\n",
      "5006\n",
      "5008\n"
     ]
    }
   ],
   "source": [
    "# TRY NOT TO MODIFY: start the game\n",
    "obs = envs.reset()\n",
    "start_time = time.time()\n",
    "for global_step in range(args.total_timesteps):\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    if global_step < args.learning_starts:\n",
    "        actions = np.array(envs.action_space.sample())\n",
    "    else:\n",
    "        actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))\n",
    "        actions = actions.detach().cpu().numpy()\n",
    "    #print('actions', actions)\n",
    "\n",
    "    # TRY NOT TO MODIFY: execute the game and log data.\n",
    "    #print('envs.step(actions)', envs.step(actions))\n",
    "    next_obs, rewards, sum_reward, terminations, steps_count, all_actions, = envs.step(actions) # removed truncations, infos \n",
    "       \n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    # if \"final_info\" in infos:\n",
    "    #     for info in infos[\"final_info\"]:\n",
    "    #         print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "    #         writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "    #         writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "    #         break\n",
    "\n",
    "    # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "    #real_next_obs = next_obs.copy()\n",
    "    # for idx, trunc in enumerate(truncations):\n",
    "    #     if trunc:\n",
    "    #         real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
    "    rb.add(obs, actions, next_obs, rewards, terminations)\n",
    "\n",
    "    # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "    obs = next_obs\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > args.learning_starts:\n",
    "        data = rb.sample(args.batch_size)\n",
    "        with torch.no_grad():\n",
    "            next_state_actions, next_state_log_pi, _ = actor.get_action(data[2])\n",
    "            #print('next_state_log_pi', next_state_log_pi.shape)\n",
    "            qf1_next_target = qf1_target(data[2], next_state_actions)\n",
    "            #print('qf1_next_target', qf1_next_target.shape)\n",
    "            qf2_next_target = qf2_target(data[2], next_state_actions)\n",
    "            #print('qf2_next_target', qf2_next_target.shape)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi\n",
    "            #print('min_qf_next_target', min_qf_next_target.shape)\n",
    "            next_q_value = data[3].flatten() + (1 - data[4].flatten()) * args.gamma * (min_qf_next_target).view(-1)\n",
    "            #print('next_q', next_q_value.shape)\n",
    "\n",
    "        qf1_a_values = qf1(data[0], data[1]).view(-1)\n",
    "        qf2_a_values = qf2(data[0], data[1]).view(-1)\n",
    "        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "        qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        # optimize the model\n",
    "        q_optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support\n",
    "            print(global_step)\n",
    "            for _ in range(args.policy_frequency):  # compensate for the delay by doing 'actor_update_interval' instead of 1\n",
    "                pi, log_pi, _ = actor.get_action(data[0])\n",
    "                qf1_pi = qf1(data[0], pi)\n",
    "                qf2_pi = qf2(data[0], pi)\n",
    "                min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "                actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                if args.autotune:\n",
    "                    with torch.no_grad():\n",
    "                        _, log_pi, _ = actor.get_action(data[0])\n",
    "                    alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()\n",
    "\n",
    "                    a_optimizer.zero_grad()\n",
    "                    alpha_loss.backward()\n",
    "                    a_optimizer.step()\n",
    "                    alpha = log_alpha.exp().item()\n",
    "\n",
    "        # update the target networks\n",
    "        if global_step % args.target_network_frequency == 0:\n",
    "            for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "        # if global_step % 100 == 0:\n",
    "        #     writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "        #     writer.add_scalar(\"losses/qf2_values\", qf2_a_values.mean().item(), global_step)\n",
    "        #     writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "        #     writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), global_step)\n",
    "        #     writer.add_scalar(\"losses/qf_loss\", qf_loss.item() / 2.0, global_step)\n",
    "        #     writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "        #     writer.add_scalar(\"losses/alpha\", alpha, global_step)\n",
    "        #     print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "        #     writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        #     if args.autotune:\n",
    "        #         writer.add_scalar(\"losses/alpha_loss\", alpha_loss.item(), global_step)\n",
    "        if global_step % 100 == 0:\n",
    "            print(f\"Step: {global_step}, QF1 Values Mean: {qf1_a_values.mean().item()}\")\n",
    "            print(f\"Step: {global_step}, QF2 Values Mean: {qf2_a_values.mean().item()}\")\n",
    "            print(f\"Step: {global_step}, QF1 Loss: {qf1_loss.item()}\")\n",
    "            print(f\"Step: {global_step}, QF2 Loss: {qf2_loss.item()}\")\n",
    "            print(f\"Step: {global_step}, QF Loss: {qf_loss.item() / 2.0}\")\n",
    "            print(f\"Step: {global_step}, Actor Loss: {actor_loss.item()}\")\n",
    "            print(f\"Step: {global_step}, Alpha: {alpha}\")\n",
    "            sps = int(global_step / (time.time() - start_time))\n",
    "            print(f\"Step: {global_step}, SPS (Samples Per Second): {sps}\")\n",
    "            if args.autotune:\n",
    "                print(f\"Step: {global_step}, Alpha Loss: {alpha_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
