{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluid Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FluidMechanicsEnv:\n",
    "    \n",
    "    class Wave:\n",
    "        def __init__(self, a, T, k) :\n",
    "            self.a = a                  # Wave amplitude\n",
    "            self.T = T                  # Wave period\n",
    "            self.omega = 2 * np.pi / T  # Wave frequency\n",
    "            self.k = .1   \n",
    "\n",
    "    class Wind:\n",
    "        def __init__(self, Ux, Uy, alpha, sigma) :\n",
    "            self.Ux = Ux                  # Wave amplitude\n",
    "            self.Uy = Uy                  # Wave period\n",
    "            self.alpha = alpha            # Wave frequency\n",
    "            self.sigma = sigma\n",
    "\n",
    "    def __init__(self, a, T, k,  Ux, Uy, alpha, sigma, x_goal, y_goal, pos0, theta0, dist_threshold=0.1, max_steps=1000):\n",
    "        self.t = 0\n",
    "        self.wave = self.Wave(a, T, k)\n",
    "        self.wind = self.Wind(Ux, Uy, alpha, sigma)\n",
    "        self.max_steps = max_steps\n",
    "        self.dist_threshold = dist_threshold\n",
    "        self.max_x, self.min_x = 100 , -100  # agent has drifted too far, admit defeat\n",
    "        self.max_y, self.min_y = 100 , -100 # agent has drifted too far, admit defeat\n",
    "        self.x_goal, self.y_goal, self.z_goal = x_goal, y_goal, 0 # coordinates of goal\n",
    "        self.done = False\n",
    "        self.goal_reached = False\n",
    "        self.steps_count = 0\n",
    "        self.sum_reward = 0\n",
    "        self.all_actions = []\n",
    "        self.pos = pos0\n",
    "        self.theta = theta0\n",
    "        self.vel = np.array([0, 0, 0]).astype(np.float32)\n",
    "        self.thrust = 0 # [0; 1]\n",
    "        self.rudder = 0.0 # [-pi/4; pi/4]\n",
    "        self.action = np.array([0, 0])\n",
    "        self.u_history = []\n",
    "        self.v_history = []\n",
    "\n",
    "        self.state_dim = 3  # x, y, z. Should add later u_swell, u_wind, v_wind, w_swell\n",
    "        self.action_dim = 2  # thrust, rudder angle\n",
    "\n",
    "    def water_surface_level(self, pos) :\n",
    "        x, _, _ = pos\n",
    "        eta = self.wave.a * np.sin(self.wave.omega * self.t - self.wave.k * x)\n",
    "        return eta\n",
    "\n",
    "    def water_speed(self, pos) :\n",
    "        x, y, z = pos\n",
    "        eta = self.water_surface_level(pos)\n",
    "\n",
    "        u_swell = self.wave.a * self.wave.omega * np.exp(self.wave.k * z) * np.sin(self.wave.omega * self.t - self.wave.k * x)\n",
    "        w_swell = self.wave.a * self.wave.omega * np.exp(self.wave.k * z) * np.cos(self.wave.omega * self.t - self.wave.k * x)\n",
    "        \n",
    "        u_wind = np.random.normal(self.wind.Ux, self.wind.sigma) * np.exp(-self.wind.alpha * (eta - z))\n",
    "        v_wind = np.random.normal(self.wind.Uy, self.wind.sigma) * np.exp(-self.wind.alpha * (eta - z))\n",
    "\n",
    "        # u = u + np.random.normal(0, noise, u.shape)\n",
    "        # v = v + np.random.normal(0, noise, v.shape)\n",
    "        # w = w + np.random.normal(0, noise, w.shape)\n",
    "\n",
    "        return u_swell + u_wind, v_wind, w_swell\n",
    "\n",
    "    def inertia(self, lag = 5) :\n",
    "\n",
    "        if len(self.u_history) > 0 :\n",
    "\n",
    "            k = np.minimum(lag, len(self.u_history))\n",
    "            coefs = np.array([1 / (4 ** (i + 1)) for i in reversed(range(k))])\n",
    "            u = (self.u_history[-k:] * coefs).sum() / coefs.sum()\n",
    "            v = (self.v_history[-k:] * coefs).sum() / coefs.sum()\n",
    "\n",
    "        else :\n",
    "            u, v = 0, 0\n",
    "\n",
    "        return np.array([u, v, 0])\n",
    "    \n",
    "    def update_pos(self, action):\n",
    "        # Sets agent action\n",
    "        self.thrust = action[0]\n",
    "        self.rudder = action[1]\n",
    "    \n",
    "        # Find the water velocity at agent position\n",
    "        x, y, z = self.pos\n",
    "        u, v, w = self.water_speed(self.pos)\n",
    "        self.vel = np.array([u, v, w])\n",
    "\n",
    "        # Add inertia to the agent's velocity\n",
    "        self.vel += self.inertia()\n",
    "\n",
    "        # Perform agent action\n",
    "        self.theta -= self.rudder # Update agent's orientation from rudder angle\n",
    "        u_action = self.thrust * np.sin(self.theta)\n",
    "        v_action = self.thrust * np.cos(self.theta)\n",
    "        self.vel += np.array([u_action, v_action, 0])\n",
    "\n",
    "        # Update velocity history\n",
    "        self.u_history.append(u)\n",
    "        self.v_history.append(v)\n",
    "\n",
    "        # Update agent position\n",
    "        x += self.vel[0]\n",
    "        y += self.vel[1]\n",
    "        z = self.water_surface_level((x, y, z))\n",
    "\n",
    "        return np.array([x, y, z])\n",
    "    \n",
    "    def get_reward(self):\n",
    "        \n",
    "        # Calculate euclidian dist to goal Without z coord\n",
    "        goal_pos = np.array([self.x_goal, self.y_goal])\n",
    "        dist_to_goal = np.linalg.norm(np.array(self.pos[:2]) - goal_pos)\n",
    "        reward = - dist_to_goal\n",
    "        if dist_to_goal <= self.dist_threshold:\n",
    "            reward += 10\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def success(self):\n",
    "        \"\"\"Returns True if x,y is near enough goal\"\"\"\n",
    "        goal_pos = np.array([self.x_goal, self.y_goal])\n",
    "        dist_to_goal = np.linalg.norm(np.array(self.pos[:2]) - goal_pos)\n",
    "        if  dist_to_goal <= self.dist_threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def admit_defeat(self):\n",
    "        \"\"\"Returns True if the agent has drifted too far away from goal\"\"\"\n",
    "        if self.pos[0] > self.max_x or self.pos[0] < self.min_x or self.pos[1] > self.max_y or self.pos[1] < self.min_y:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def step(self, action) :\n",
    "        \n",
    "        self.pos = self.update_pos(action)\n",
    "        self.reward = self.get_reward()\n",
    "        self.sum_reward += self.reward\n",
    "        self.steps_count += 1\n",
    "        self.all_actions += [action]\n",
    "\n",
    "        if self.success():\n",
    "            self.done = True\n",
    "            self.goal_reached = True\n",
    "\n",
    "        elif self.admit_defeat() or self.steps_count > self.max_steps:\n",
    "            self.done = True\n",
    "\n",
    "        return self.pos, self.reward, self.sum_reward, self.done, self.steps_count, self.all_actions\n",
    "    \n",
    "    def reset(self):\n",
    "\n",
    "        self.rudder = 0\n",
    "        self.thrust = 0  \n",
    "        self.pos = np.array([0, 0, 0])\n",
    "        self.done = False\n",
    "        self.goal_reached = False\n",
    "        self.steps_count = 0\n",
    "        self.sum_reward = 0\n",
    "        \n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.70710678,  0.70710678, -0.        ]),\n",
       " -0.41421356237309503,\n",
       " -0.41421356237309503,\n",
       " False,\n",
       " 1,\n",
       " [array([ 1.        , -0.78539816])])"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = FluidMechanicsEnv(a=0,\n",
    "                        T=1,\n",
    "                        k=0.1,\n",
    "                        Ux=0,\n",
    "                        Uy=0,\n",
    "                        alpha=1, \n",
    "                        sigma=0, \n",
    "                        x_goal=1,\n",
    "                        y_goal=1, \n",
    "                        pos0=np.array([0, 0, 0]), \n",
    "                        theta0=0, \n",
    "                        dist_threshold=0.1, \n",
    "                        max_steps=1000)\n",
    "\n",
    "action = np.array([1, -np.pi/4])\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.98994949,  0.98994949, -0.        ]),\n",
       " 9.985786437626905,\n",
       " 9.571572875253809,\n",
       " True,\n",
       " 2,\n",
       " [array([ 1.        , -0.78539816]), array([0.4, 0. ])])"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = np.array([.4, 0])\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "\n",
    "class ACModel(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_size=64):\n",
    "        super(ACModel, self).__init__()\n",
    "        # Common hidden layer\n",
    "        self.common = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Actor - Output parameters for the distributions\n",
    "        self.actor_thrust = nn.Linear(hidden_size, 2)  # Parameters for Beta distribution (alpha, beta)\n",
    "        self.actor_rudder = nn.Linear(hidden_size, 2)  # Parameters for Gaussian distribution (mean, std_dev)\n",
    "        # Critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.common(state)\n",
    "        # Thrust\n",
    "        thrust_params = torch.exp(self.actor_thrust(x))  # Ensure parameters are positive\n",
    "        thrust_dist = dist.Beta(thrust_params[:, 0]+1, thrust_params[:, 1]+1)  # Adding 1 to avoid 0\n",
    "        # Rudder\n",
    "        rudder_params = torch.exp(self.actor_rudder(x))\n",
    "        rudder_dist = dist.Beta(rudder_params[:, 0]+1, rudder_params[:, 1]+1)\n",
    "\n",
    "        # Compute value\n",
    "        value = self.critic(state)\n",
    "        return thrust_dist, self.rescale_beta(rudder_dist, -np.pi/4, np.pi/4), value\n",
    "\n",
    "    def rescale_beta(self, beta_dist, low, high):\n",
    "        \"\"\"\n",
    "        Rescale a Beta distribution to a new interval [low, high].\n",
    "        \"\"\"\n",
    "        def sample_rescaled(*args, **kwargs):\n",
    "            samples = beta_dist.sample(*args, **kwargs)\n",
    "            return low + (high - low) * samples\n",
    "\n",
    "        def log_prob_rescaled(samples):\n",
    "            # Adjust samples to original Beta scale\n",
    "            original_samples = (samples - low) / (high - low)\n",
    "            # Compute log_prob on the original scale, adjust for the scale transformation\n",
    "            return beta_dist.log_prob(original_samples) - torch.log(torch.tensor(high - low))\n",
    "        def entropy_rescaled():\n",
    "            scale = high - low\n",
    "            return beta_dist.entropy() + torch.log(torch.tensor(scale, dtype=torch.float))\n",
    "\n",
    "        # Return a simple object with adjusted sample, log_prob, and entropy methods\n",
    "        return type('RescaledBeta', (object,), {\n",
    "            'sample': sample_rescaled,\n",
    "            'log_prob': log_prob_rescaled,\n",
    "            'entropy': entropy_rescaled\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                max_episodes=500,\n",
    "                gae_lambda=0.95,\n",
    "                use_critic=False,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=False,\n",
    "                entropy_coef=0.01,\n",
    "                use_gae=False):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.max_episodes = max_episodes # the maximum number of episodes.\n",
    "        self.use_critic = use_critic # whether to use critic or not.\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.entropy_coef = entropy_coef # entropy coefficient for PPO\n",
    "        self.use_gae = use_gae # whether to use GAE or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discounted_return(rewards, discount, device=None):\n",
    "    \"\"\"\n",
    "\t\trewards: reward obtained at timestep.  Shape: (T,)\n",
    "\t\tdiscount: discount factor. float\n",
    "\n",
    "    ----\n",
    "    returns: sum of discounted rewards. Shape: (T,)\n",
    "\t\t\"\"\"\n",
    "    returns = torch.zeros(*rewards.shape, device=device)\n",
    "\n",
    "    R = 0\n",
    "    for t in reversed(range((rewards.shape[0]))):\n",
    "        R = rewards[t] + discount * R\n",
    "        returns[t] = R\n",
    "    return returns\n",
    "\n",
    "def compute_advantage_gae(values, rewards, T, gae_lambda, discount):\n",
    "    \"\"\"\n",
    "    Compute Adavantage wiht GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "    values: value at each timestep (T,)\n",
    "    rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "    T: the number of frames, float\n",
    "    gae_lambda: hyperparameter, float\n",
    "    discount: discount factor, float\n",
    "\n",
    "    -----\n",
    "\n",
    "    returns:\n",
    "\n",
    "    advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                 gae advantage term for timesteps 0 to T\n",
    "\n",
    "    \"\"\"\n",
    "    advantages = torch.zeros_like(values)\n",
    "    for i in reversed(range(T)):\n",
    "        next_value = values[i+1]\n",
    "        next_advantage = advantages[i+1]\n",
    "\n",
    "        delta = rewards[i] + discount * next_value  - values[i]\n",
    "        advantages[i] = delta + discount * gae_lambda * next_advantage\n",
    "    return advantages[:T]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experiences(env, acmodel, args, device=None):\n",
    "    \"\"\"Collects rollouts and computes advantages.\n",
    "    Returns\n",
    "    -------\n",
    "    exps : dict\n",
    "        Contains actions, rewards, advantages etc as attributes.\n",
    "        Each attribute, e.g. `exps['reward']` has a shape\n",
    "        (self.num_frames, ...).\n",
    "    logs : dict\n",
    "        Useful stats about the training process, including the average\n",
    "        reward, policy loss, value loss, etc.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    MAX_FRAMES_PER_EP = 300\n",
    "    shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "    actions = torch.zeros((MAX_FRAMES_PER_EP, 2), device=device, dtype=torch.int)\n",
    "    values = torch.zeros(*shape, device=device)\n",
    "    rewards = torch.zeros(*shape, device=device)\n",
    "    log_probs = torch.zeros(*shape, device=device)\n",
    "    #obss = [None]*MAX_FRAMES_PER_EP\n",
    "    obss = torch.zeros((MAX_FRAMES_PER_EP, 3), device=device)\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    total_return = 0\n",
    "\n",
    "    T = 0\n",
    "\n",
    "    while True:\n",
    "        # Do one agent-environment interaction\n",
    "\n",
    "        with torch.no_grad():\n",
    "            obs = torch.from_numpy(obs).float()\n",
    "            obs = obs.unsqueeze(0)\n",
    "            thrust_dist, rudder_dist, value = acmodel(obs)\n",
    "        action = torch.stack((thrust_dist.sample(), rudder_dist.sample()), dim=-1).squeeze()\n",
    "        #print(action)\n",
    "        obss[T] = obs\n",
    "        obs, reward,  _, done, _, _ = env.step(action)\n",
    "\n",
    "        # Update experiences values\n",
    "        actions[T] = action\n",
    "        values[T] = value\n",
    "        rewards[T] = reward\n",
    "        print(thrust_dist.log_prob(action[0]) + rudder_dist.log_prob(action[1]))\n",
    "        log_probs[T] = thrust_dist.log_prob(action[0]) + rudder_dist.log_prob(action[1])\n",
    "\n",
    "        total_return += reward\n",
    "        T += 1\n",
    "\n",
    "        if done or T>=MAX_FRAMES_PER_EP-1:\n",
    "            break\n",
    "\n",
    "    discounted_reward = compute_discounted_return(rewards[:T], args.discount, device)\n",
    "    exps = dict(\n",
    "        obs = obss[:T],\n",
    "        action = actions[:T],\n",
    "        value  = values[:T],\n",
    "        reward = rewards[:T],\n",
    "        advantage = discounted_reward-values[:T],\n",
    "        log_prob = log_probs[:T],\n",
    "        discounted_reward = discounted_reward,\n",
    "        advantage_gae=compute_advantage_gae(values, rewards, T, args.gae_lambda, args.discount)\n",
    "    )\n",
    "\n",
    "    logs = {\n",
    "        \"return_per_episode\": total_return,\n",
    "        \"num_frames\": T\n",
    "    }\n",
    "\n",
    "    return exps, logs\n",
    "\n",
    "def run_experiment(args, parameter_update, env_param, seed=0):\n",
    "    \"\"\"\n",
    "    Upper level function for running experiments to analyze reinforce and\n",
    "    policy gradient methods. Instantiates a model, collects epxeriences, and\n",
    "    then updates the neccessary parameters.\n",
    "\n",
    "    args: Config arguments. dict\n",
    "    paramter_update: function used to update model parameters\n",
    "    seed: random seed. int\n",
    "\n",
    "    return: DataFrame indexed by episode\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = FluidMechanicsEnv(**env_param)\n",
    "\n",
    "    acmodel = ACModel(env.state_dim)\n",
    "    acmodel.to(device)\n",
    "\n",
    "    is_solved = False\n",
    "\n",
    "    SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "    optimizer = torch.optim.Adam(acmodel.parameters(), lr=args.lr)\n",
    "    num_frames = 0\n",
    "\n",
    "    pbar = tqdm(range(args.max_episodes))\n",
    "    for update in pbar:\n",
    "        exps, logs1 = collect_experiences(env, acmodel, args, device)\n",
    "        logs2 = parameter_update(optimizer, acmodel, exps, args)\n",
    "\n",
    "        logs = {**logs1, **logs2}\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "\n",
    "        rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"]}\n",
    "\n",
    "        if args.use_critic:\n",
    "            data['value_loss'] = logs[\"value_loss\"]\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        # Early terminate\n",
    "        if smooth_reward >= args.score_threshold:\n",
    "            is_solved = True\n",
    "            break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "\n",
    "    return pd.DataFrame(pd_logs).set_index('episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_ppo(optimizer, acmodel, sb, args):\n",
    "    def _compute_policy_loss_ppo(obs, old_logp, actions, advantages):\n",
    "        '''\n",
    "        Computes the policy loss for PPO.\n",
    "\n",
    "        obs: observeration to pass into acmodel. shape: (T,)\n",
    "        old_logp: log probabilities from previous timestep. shape: (T,)\n",
    "        actions: action at this timestep. shape: (T,ImWidth,ImHeight,Channels)\n",
    "        advantages: the computed advantages. shape: (T,)\n",
    "\n",
    "        ---\n",
    "        returns\n",
    "\n",
    "        policy_loss : ppo policy loss as shown in line 6 of PPO alg. tensor.float. Shape (,1)\n",
    "        approx_kl: an appoximation of the kl_divergence. tensor.float. Shape (,1)\n",
    "        '''\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        ### TODO: implement PPO policy loss computation (30 pts).  #######\n",
    "\n",
    "        # Policy loss\n",
    "        T = len(obs)\n",
    "        eps = args.clip_ratio\n",
    "        thrust_dist, rudder_dist, _ = acmodel(obs)\n",
    "        print(actions)\n",
    "        #print(thrust_dist.log_prob(actions[:,0]))\n",
    "        logp = thrust_dist.log_prob(actions[:,0]) + rudder_dist.log_prob(actions[:,1])\n",
    "        #print(actions)\n",
    "        #print(logp)\n",
    "\n",
    "        for t in range(T):\n",
    "            if advantages[t] >= 0:\n",
    "              g = (1+eps)*advantages[t]\n",
    "            else:\n",
    "              g = (1-eps)*advantages[t]\n",
    "\n",
    "            policy_loss -= torch.min(g, logp[t].exp()/old_logp[t].exp()*advantages[t])\n",
    "\n",
    "        # Add entropy\n",
    "        entropy = thrust_dist.entropy() + rudder_dist.entropy() \n",
    "        policy_loss -= args.entropy_coef*entropy.sum()\n",
    "\n",
    "        # Normlaize\n",
    "        policy_loss = policy_loss/T\n",
    "\n",
    "        # KL oldprobs / new probs\n",
    "        for t in range(T):\n",
    "          r = logp[t].exp()/old_logp[t].exp()\n",
    "          approx_kl += (r-1) - r.log()\n",
    "\n",
    "        ##################################################################\n",
    "\n",
    "        return policy_loss, approx_kl\n",
    "\n",
    "    def _compute_value_loss(obs, returns):\n",
    "        ### TODO: implement PPO value loss computation (10 pts) ##########\n",
    "\n",
    "        _, _, values = acmodel(obs)\n",
    "        value_loss = F.mse_loss(values.squeeze(),returns)\n",
    "        ##################################################################\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    #print(sb['obs'])    \n",
    "    thrust_dist, rudder_dist, _ = acmodel(sb['obs'])\n",
    "    old_logp = thrust_dist.log_prob(sb['action'][:,0]).detach() + rudder_dist.log_prob(sb['action'][:,1]).detach()\n",
    "\n",
    "    advantage = sb['advantage_gae'] if args.use_gae else sb['advantage']\n",
    "\n",
    "    policy_loss, _ = _compute_policy_loss_ppo(sb['obs'], old_logp, sb['action'], advantage)\n",
    "    value_loss = _compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "\n",
    "    for i in range(args.train_ac_iters):\n",
    "        optimizer.zero_grad()\n",
    "        loss_pi, approx_kl = _compute_policy_loss_ppo(sb['obs'], old_logp, sb['action'], advantage)\n",
    "        loss_v = _compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
    "\n",
    "        loss = loss_v + loss_pi\n",
    "        if approx_kl > 1.5 * args.target_kl:\n",
    "            break\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    update_policy_loss = policy_loss.item()\n",
    "    update_value_loss = value_loss.item()\n",
    "\n",
    "    logs = {\n",
    "        \"policy_loss\": update_policy_loss,\n",
    "        \"value_loss\": update_value_loss,\n",
    "    }\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_param = dict(\n",
    "    a=0,\n",
    "    T=1,\n",
    "    k=0.1,\n",
    "    Ux=0,\n",
    "    Uy=0,\n",
    "    alpha=1, \n",
    "    sigma=0, \n",
    "    x_goal=1,\n",
    "    y_goal=1, \n",
    "    pos0=np.array([0, 0, 0]), \n",
    "    theta0=0, \n",
    "    dist_threshold=0.1, \n",
    "    max_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config(use_critic=True, use_gae=True)\n",
    "#df_ppo = run_experiment(args, update_parameters_ppo, env_param)\n",
    "#df_ppo.plot(x='num_frames', y=['reward', 'smooth_reward'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
